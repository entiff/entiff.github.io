---
layout: post
title:  "ALBERT"
date:   2019-10-14 21:00:00
author: entiff
categories: NLP
---

대표적인 QA(Question Answering) 데이터셋인 [SQuAD 1.1](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/)에서 새로운 SOTA 모델이 등장했습니다.

[ALBERT](https://arxiv.org/pdf/1909.11942.pdf)는 기존의 SOTA 모델들과 유사하게 BERT를 기본으로 하지만 parameter의 수가 적다는 점이 가장 큰 특징입니다.

이번 글에서는 ALBERT의 핵심 내용을 짧게 요약합니다.

### 1. Factorized embedding parameterization

![transformer_embedding](https://machinetalk.org/wp-content/uploads/2019/04/encoder.png)

위 그림에서 inputs은 input embedding으로 변환되어 Positional Embeeding과 더해지고 Query, Key, Value로 projection됩니다. 이때 Input Embedding size는 Query, Key, Value embedding size와 동일합니다.

ALBERT의 저자들은 context가 반영되어 있지 않은 Input embedding이 suboptimal 상태에 있다고 봤습니다. Vocab size = V, Input Embedding size = E, Q/K/V embedding size = H라고 했을 때 Input Embedding Size는 V x H가 아니라 V x E + E X H의 형태로 Factorization합니다.

만약 H > E라면 parameter를 크게 줄일 수 있습니다.

### 2. Cross-layer parameter sharing

BERT에서는 Transforemr encoder를 여러 Layer로 쌓는 것이 핵심입니다. 이때, Transformer encoder에는 Multi-Head Attention, Feed Forward 부분에 parameter가 있고 Layer끼리 parameter를 공유하지 않기 때문에 Layer를 쌓을수록 parameter가 크게 증가합니다.

ALBERT의 저자들은 'Layer의 차이에 따라 input과 output embedding의 차이가 일정하다면 parameter를 공유해도 괜찮을 것이다'라는 가설을 세웁니
다.

![ALBERT_input_output](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fcab7045f-31a2-4c7e-b3c6-2fdb290f1598%2FUntitled.png?table=block&id=ebfc3bf7-53c9-4f24-8075-92e3c90be3f9&width=2280&cache=v2)

### 3. Inter-sentence coherence visualcommonsense

BERT의 Pre-trained task 중 하나인 Next Sentence Prediction(NSP)는 후속 연구에서 효과적이지 못하다는 결과가 지속적으로 나왔습니다. ALBERT의 저자들 역시 NSP task의 한계를 언급하며 좀 더 어려운 task를 추가합니다.

아이디어는 간단합니다. NSP에서 negative sample을 random sentence가 아니라 순서를 뒤집은 것으로 만들고 이를 Sentence Order Prediction이라고 이름 지었습니다.

다시 말해, 기존의 Corpus에서 Sentence1, Sentence2로 있던 문장을 Sentence2, Sentence1으로 넣고 negative sample을 만듭니다. 간단하지만 NSP task에 비해 어렵고 문장구조를 학습하는 데에 좋은 효과를 냈습니다.
