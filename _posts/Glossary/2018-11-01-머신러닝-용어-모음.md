---
layout: post
title:  "머신러닝 용어 모음(Machine Learning Glossary)"
date:   2018-11-01 22:00:00
author: 김태희
categories: Glossary
use_math: true
---

머신러닝 알고리즘 용어 모음집을 작성 중입니다.  
여러분들은 아래의 용어에 대해 모두 설명할 수 있으신가요?  
아래의 용어들에 대해 모두 간단하고 쉽게 설명해보는 것을 목표로 공부하면 좋을 것 같습니다 :)  
오타 혹은 오개념에 대해서는 댓글로 남겨주시면 감사하겠습니다.  
지속적인 업데이트가 이루어지고 있습니다.  

피드백은 언제나 환영입니다.

# Methods

## 1. Linear Regression(선형 회귀)

예측 문제에 자주 사용됩니다.
선형회귀는 종속 변수(Dependent Variable) y와 한 개 이상의 독립 변수(Independent Variable) X와의 선형 관계를 모델링하는 기법입니다.
단순 선형 회귀의 경우 1개의 독립 변수, 다중 선형 회귀의 경우 2개 이상의 독립 변수를 갖습니다.
종속 변수 y가 여러개일 경우 다변량 선형 회귀라 부릅니다.

Parameters 또는 Weight 등으로 불리우는 $\beta$ 를 추정하는 방식은 크게 최소제곱법(OLS)과 최대우도법(MLE)으로 나뉩니다.
Machine Learning에서는 오차제곱합으로 나타낸 Loss function을 최소화하는 방식으로 parameter를 추정합니다.

선형 회귀 분석의 기본 가정은 다음과 같습니다.
1. 잔차항(residuals, 오차항)은 정규성(평균 = 0), 등분산성(분산은 ${ \sigma  }^{ 2 }$), 독립성(오차항은 서로 독립)을 가정한다.
2. 수집된 데이터의 확률 분포는 정규분포를 이루고 있다.
3. 독립변수 상호간에는 상관관계가 없어야 한다. (아닐 경우 Multi-colinearity 문제 발생)
4. 독립변수와 종속변수는 선형적인 관계를 갖고 있다.

### 1-1. Simple Linear Regression(단순 선형 회귀)

$$ y = \beta x + \varepsilon $$

### 1-2. Multi Linear Regression(중다 회귀, 다중 회귀)

$$ y = { \beta  }_{ 1 }{ x }_{ 1i } + ... + { \beta  }_{ i }{ x }_{ ip } + \varepsilon = { X }_{ i }^{ T }\beta + { \varepsilon  }_{ i } $$

### 1-3. Generalized Linear Model(일반화 선형 모델)

종속변수와 독립변수 모두 연속형 변수일 경우 선형 모형을 사용할 수 있으나 종속 변수가
이산형(Discrete) 변수일 경우, Generalized Linear Model을 이용해 분석합니다.
종속변수가 연결함수(Link Function)를 통해 독립변수와 선형적인 관계를 갖도록 합니다.
대표적으로 Logistic Regression, Poisson Regression이 있습니다.

선형 모형을 $y = ax + b$, 연결함수를 $g()$라 가정할 때,
$g(y) = ax + b$로 바꾸어 $g(y)$와 $x$의 관계를 선형적으로 해석합니다.
$y$와 $x$의 관계를 선형적이지 않고 Link Function인 $g()$에 의해 바뀌기 때문에 주의해야 합니다.

* Concept Note
  - 1. Multi-colinearity(다중공선성)
  - 2. Ordinary Least Square(OLS)
  - 3. Maximum Likelihood Estimation(MLE)

## 2. Logistic Regression

로지스틱 회귀는 분류 문제에 자주 쓰입니다.
종속 변수 $y$ 가 ${0,1}$ 또는 ${0,1,2,3 ... , n}$ 등으로 표현되는 범주형 변수의 경우
로지스틱 회귀를 사용하고 다음과 같이 모델을 표현합니다.  

$${ h }_{ \theta  }(x)\quad =\quad \frac { 1 }{ 1\quad +\quad { e }^{ -{ \theta  }^{ T }x } }$$

![로지스틱 함수](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png){: width="300" height="300"}

기본적으로는 threshold 0.5를 기준으로 1 또는 0으로 분류합니다.

Cost function은 다음과 같습니다.
$$-\frac { 1 }{ m } [\sum _{ i=1 }^{ m }{ { y }^{ (i) }log{ h }_{ \theta  }({ x }^{ (i) })+(1-{ y }^{ (i) })log(1-{ h }_{ \theta  }({ x }^{ (i) }))] }$$

다중 분류 문제의 경우, 여러개의 Classifier를 학습하여 풀어냅니다.
4개의 집단을 분류하는 Classifier를 학습하는 경우,
1. 1번 집단과 나머지 집단을 구분하는 분류기
2. 2번 집단과 나머지 집단을 구분하는 분류기
3. 3번 집단과 나머지 집단을 구분하는 분류기
4. 4번 집단과 나머지 집단을 구분하는 분류기

이렇게 4개의 분류기를 통해 다중 분류 문제를 풀어냅니다.

## 3. Support Vector Machine(SVM)

### 3-1. Soft Margin SVM

### 3-2. SVM with Kernel Trick

## 4. Decision Tree(의사결정 나무)

## 5. Random Forest

## 6. K-means Clustering

## 7. Latent Semantic Analysis(LSA)

* Concept Note
 - Singular Value Decomposition(SVD)

## 8. Latent Dirichlet Allocation(LDA)

* Concept Note
 - Dirichlet Distribution
 - truncked SVD
## 9. Naive Bayes

## 10. Boosting

### 10-1. xgboost
### 10-2. LightGBM
### 10-3. Catboost

* Concpet Note
 - Ensemble

## 11. Bagging

* Concept Note
 - Ensemble

## 13. Ensemble(앙상블)

## 14. Bias-Variance Trade Off

# Dimension Reduction(차원 축소)

## 1. Principal Component Analysis(PCA)

* Concept Note
 - Singular Value Decomposition
 - Eigen Value, Eigen Vector

## 2. t-Distributed Stochastic Neighbor Embedding(t-SNE)

* Concept Note
 - Student's t-distribution(t-distribution, gaussian distribution)

## 3. Auto-Encoder

# Technic

## 1. Cross Validation

## 2. Regularization

### 2-1. Ridge

### 2-2. Rasso

# Metric

## 1. RMSE(Root Mean Square Error)

## 2. MSE(Mean Square Error)

## 3. Accuracy

## 4. Precision

## 5. Recall(Sensitivity, TP Rate, Hit Rate)

## 6. F-1 score

## 7. ROC curve

###### etc
- Markov Chain
- Association Rule
  - Support
  - Confidence
  - Lift
