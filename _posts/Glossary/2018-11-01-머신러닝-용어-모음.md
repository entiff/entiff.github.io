---
layout: post
title:  "머신러닝 용어 모음(Machine Learning Glossary)"
date:   2018-11-01 22:00:00
author: 김태희
categories: Glossary
---

머신러닝 알고리즘 용어 모음집을 작성 중입니다.  
여러분들은 아래의 용어에 대해 모두 설명할 수 있으신가요?  
아래의 용어들에 대해 모두 간단하고 쉽게 설명해보는 것을 목표로 공부하면 좋을 것 같습니다 :)  
오타 혹은 오개념에 대해서는 댓글로 남겨주시면 감사하겠습니다.  
지속적인 업데이트가 이루어지고 있습니다.  

피드백은 언제나 환영입니다.

# Methods

## 1. Linear Regression(선형 회귀)

예측 문제에 자주 사용됩니다.
선형회귀는 종속 변수(Dependent Variable) $y$와 한 개 이상의 독립 변수(Independent Variable) $X$와의 선형 관계를 모델링하는 기법입니다.
단순 선형 회귀의 경우 1개의 독립 변수, 다중 선형 회귀의 경우 2개 이상의 독립 변수를 갖습니다.
종속 변수 $y$가 여러개일 경우 다변량 선형 회귀라 부릅니다.

Parameters 또는 Weight 등으로 불리우는 $\beta$ 를 추정하는 방식은 크게 최소제곱법(OLS)과 최대우도법(MLE)으로 나뉩니다.
Machine Learning에서는 오차제곱합으로 나타낸 Loss function(손실 함수, 비용 함수, 목적 함수)을 최소화하는 방식으로 Parameter를 추정합니다.

선형 회귀 분석의 기본 가정은 다음과 같습니다.
1. 잔차항(residuals, 오차항)은 정규성(평균 = 0), 등분산성(분산은 ${ \sigma  }^{ 2 }$), 독립성(오차항은 서로 독립)을 가정한다.
2. 수집된 데이터의 확률 분포는 정규분포를 이루고 있다.
3. 독립변수 상호간에는 상관관계가 없어야 한다. (아닐 경우 Multi-colinearity(다중공선성) 문제 발생)
4. 독립변수와 종속변수는 선형적인 관계를 갖고 있다.

### 1-1. Simple Linear Regression(단순 선형 회귀)

$$ y = \beta x + \varepsilon $$

### 1-2. Multi Linear Regression(중다 회귀, 다중 회귀)

$$ y = { \beta  }_{ 1 }{ x }_{ 1i } + ... + { \beta  }_{ i }{ x }_{ ip } + \varepsilon = { X }_{ i }^{ T }\beta + { \varepsilon  }_{ i } $$

### 1-3. Generalized Linear Model(일반화 선형 모델)

종속변수와 독립변수 모두 연속형 변수일 경우 선형 모형을 사용할 수 있으나 종속 변수가
이산형(Discrete) 변수일 경우, Generalized Linear Model을 이용해 분석합니다.
종속변수가 연결함수(Link Function)를 통해 독립변수와 선형적인 관계를 갖도록 합니다.
대표적으로 Logistic Regression(로지스틱 회귀), Poisson Regression(포아송 회귀)이 있습니다.

선형 모형을 $y = ax + b$, 연결함수를 $g()$라 가정할 때,
$g(y) = ax + b$로 바꾸어 $g(y)$와 $x$의 관계를 선형적으로 해석합니다.
$y$와 $x$의 관계를 선형적이지 않고 Link Function인 $g()$에 의해 바뀌기 때문에 주의해야 합니다.

* Concept Note
  1. Multi-colinearity
  2. Ordinary Least Square(OLS)
  3. Maximum Likelihood Estimation(MLE)

## 2. Logistic Regression

로지스틱 회귀는 분류 문제에 자주 쓰입니다.
종속 변수 $y$ 가 ${0,1}$ 또는 ${0,1,2,3 ... , n}$ 등으로 표현되는 범주형 변수의 경우
로지스틱 회귀를 사용하고 다음과 같이 모델을 표현합니다.  

$${ h }_{ \theta  }(x)\quad =\quad \frac { 1 }{ 1\quad +\quad { e }^{ -{ \theta  }^{ T }x } }$$

![로지스틱 함수](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png){: width="200" height="300"}

기본적으로는 threshold 0.5를 기준으로 1 또는 0으로 분류합니다.

Cost function은 다음과 같습니다.

$$-\frac { 1 }{ m } [\sum _{ i=1 }^{ m }{ { y }^{ (i) }log{ h }_{ \theta  }({ x }^{ (i) })+(1-{ y }^{ (i) })log(1-{ h }_{ \theta  }({ x }^{ (i) }))] }$$

다중 분류 문제의 경우, 여러개의 Classifier(분류기)를 학습하여 풀어냅니다.
4개의 집단을 분류하는 Classifier를 학습하는 경우,
1. 1번 집단과 나머지 집단을 구분하는 Classifier
2. 2번 집단과 나머지 집단을 구분하는 Classifier
3. 3번 집단과 나머지 집단을 구분하는 Classifier
4. 4번 집단과 나머지 집단을 구분하는 Classifier

이렇게 4개의 분류기를 통해 다중 분류 문제를 풀어냅니다.

## 3. Support Vector Machine(SVM)

SVM은 다음의 그림처럼 Margin을 최대화하는 Hyperplane(Decision Boundary)를 찾는 것이 목적입니다.
또한, Logistic Regression을 변형하여 사용합니다.
따라서 Binary Classification(이진 분류) 문제에 자주 사용됩니다.

![Support Vector Machine](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS1IQUL_6FPoOQEzmwZBhX2JAhAokXfdJ_9te01U2qE2sNEBoTMAA){: width="200" height="300"}

SVM 역시 다음과 같은 모델을 상정합니다.
$${ \theta  }^{ T }{ x }^{ (i) }\ge 0$$이면 1, $${ \theta  }^{ T }{ x }^{ (i) }\le 0$$이면 0으로 분류합니다.

$${ h }_{ \theta  }(x)\quad =\quad \frac { 1 }{ 1\quad +\quad { e }^{ -{ \theta  }^{ T }x } }$$

그러나, Cost function을 다음과 같이 변경합니다.

![Cost function of SVM](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[12].png)

그림을 다시 수식으로 나타내면 다음과 같습니다.

$$C\sum _{ i=1 }^{ m }{ [{ y }^{ (i) }{ cost }_{ 1 }({ \theta  }^{ T }{ x }^{ (i) })+(1-{ y }^{ (i) }{ cost }_{ 0 }{ \theta  }^{ T }{ x }^{ (i) })]\quad +\quad \frac { 1 }{ 2 } \sum _{ i=1 }^{ n }{ { \theta  }_{ j }^{ 2 } }  } $$

첫번째 항의 상수 C는 hyper parameter, 두번째 항은 Regularization term입니다.
Regularization term을 여기서는 Ridge를 사용했습니다.
C가 너무 큰 경우 Under-fitting, 너무 작은 경우에는 Over-fitting문제가 발생합니다.

이제 이 식을 최소화하는 것으로 hyperplane을 찾게 됩니다.
hyperplane은 $\theta $ 벡터에 대해 수직인 벡터로 설정됩니다.
${ \theta  }_{ 0 }=0$이면 원점에서 $\theta $ 벡터에 대해 수직 벡터가 hyperplane이 됩니다.

### 3-1. Soft Margin SVM

위에서 설명한 C라는 parameter를 조정하여 soft한 margin을 설정할 수 있습니다.
C를 작게 설정할 경우 일반화 능력이 커지지만 under-fitting의 문제가 발생합니다.
C를 크게 설정할 경우 일반화 능력이 떨어지고 over-fitting문제가 발생합니다.
따라서, grid search 등을 통해 적절한 C를 찾아야 합니다.

### 3-2. SVM with Kernel Trick

일반적인 SVM의 경우 hyperplane은 직선으로만 설정되기 때문에 비선형 분류 문제를 푸는 데에 어려움이 있습니다.  
이에 등장한 것이 바로 Kernel trick입니다. 개념은 매우 간단합니다.

${ \theta  }_{ 0 }+{ \theta  }_{ 1 }{ x }_{ 1 }+...+{ \theta  }_{ n }{ x }_{ n }$에서 ${ x }_{ (i) }$ 를 비선형 함수에 매핑시키는 것입니다.

Kernel trick으로 새롭게 정의된 식은 다음과 같습니다.

$${ \theta  }_{ 1 }{ f }_{ 1 }+{ \theta  }_{ 2 }{ f }_{ 2 }+...+{ \theta  }_{ n }{ f }_{ n }$$

${ \theta  }_{ 1 }{ f }_{ 1 }+{ \theta  }_{ 2 }{ f }_{ 2 }+...+{ \theta  }_{ n }{ f }_{ n }\ge 0$ 이면 1, ${ \theta  }_{ 1 }{ f }_{ 1 }+{ \theta  }_{ 2 }{ f }_{ 2 }+...+{ \theta  }_{ n }{ f }_{ n }\le 0$ 이면 0으로 분류합니다.

${ f }_{ i }$는 다음과 같이 정의됩니다.

$${ f }_{ i } = similarity(x,{ l }^{ (i) })\quad =\quad exp(\frac { \left\| x-{ l }^{ (i) } \right\| ^{ 2 } }{ 2{ \sigma  }^{ 2 } } )$$

이때 $\sigma $는 hypter parameter가 되고 ${ l }^{ (i) }={ x }^{ (i) }$가 됩니다.
${ l }^{ (i) }$마다의 모든 $x$값에 대해 similarity를 구하고 optimize합니다.
$\sigma $가 큰 경우 under-fitting, 작은 경우 over-fitting의 문제가 생깁니다.

결론적으로는 $\sigma $와 $C$의 값을 잘 조정해 가장 적절한 parameter값을 찾아야 합니다.

SVM 역시 Logistic Regression과 똑같이 Multi-class Classification 문제를 해결합니다.

## 4. Decision Tree(의사결정 나무)

## 5. Random Forest

## 6. K-means Clustering

## 7. Latent Semantic Analysis(LSA)

* Concept Note
 - Singular Value Decomposition(SVD)

## 8. Latent Dirichlet Allocation(LDA)

* Concept Note
 - Dirichlet Distribution
 - truncked SVD
## 9. Naive Bayes

## 10. Boosting

### 10-1. xgboost
### 10-2. LightGBM
### 10-3. Catboost

* Concpet Note
 - Ensemble

## 11. Bagging

* Concept Note
 - Ensemble

## 13. Ensemble(앙상블)

## 14. Bias-Variance Trade Off

# Dimension Reduction(차원 축소)

## 1. Principal Component Analysis(PCA)

* Concept Note
 - Singular Value Decomposition
 - Eigen Value, Eigen Vector

## 2. t-Distributed Stochastic Neighbor Embedding(t-SNE)

* Concept Note
 - Student's t-distribution(t-distribution, gaussian distribution)

## 3. Auto-Encoder

# Technic

## 1. Cross Validation

## 2. Regularization

### 2-1. Ridge

### 2-2. Rasso

# Metric

## 1. RMSE(Root Mean Square Error)

## 2. MSE(Mean Square Error)

## 3. Accuracy

## 4. Precision

## 5. Recall(Sensitivity, TP Rate, Hit Rate)

## 6. F-1 score

## 7. ROC curve

###### etc
- Markov Chain
- Association Rule
  - Support
  - Confidence
  - Lift
